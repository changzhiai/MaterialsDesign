{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import cho_factor,cho_solve,cholesky,inv,solve_triangular\n",
    "import copy\n",
    "\n",
    "    \n",
    "class kernel_SE:\n",
    "    def __init__(self,length=1,k0=1):\n",
    "        \"The kernel as the squared exponential kernel or a gaussian distribution\"\n",
    "        self.length=length\n",
    "        self.k0=k0\n",
    "        \n",
    "    def matrix(self,x_data1,x_data2,dis_m=None):\n",
    "        \"Make the kernel matrix from two sets of data\"\n",
    "        if dis_m is None:\n",
    "            dist2=np.sum(x_data1**2,axis=1).reshape(-1,1)+np.sum(x_data2**2,axis=1)-2*np.dot(x_data1, x_data2.T)\n",
    "        else:\n",
    "            dist2=np.sum(dis_m**2,axis=0)\n",
    "        return self.k0*np.exp(-0.5*dist2/(self.length**2))\n",
    "    \n",
    "\n",
    "class GP_reg:\n",
    "    def __init__(self,kernel=kernel_SE(),noise=0.0,yp=0):\n",
    "        \"\"\"The Gaussian process that uses a kernel. \n",
    "        It can predict values if a training set and prediction features are given\"\"\"\n",
    "        self.kernel=copy.deepcopy(kernel)\n",
    "        self.noise=noise\n",
    "        self.yp=yp\n",
    "        \n",
    "    def train(self,x_train,y_train):\n",
    "        \"Train the Gaussian process with training set\"\n",
    "        self.x_train=np.copy(x_train)\n",
    "        y_tr=y_train-self.yp\n",
    "        KXX=self.kernel.matrix(x_train,x_train)\n",
    "        KXX=KXX+self.noise*np.identity(len(KXX))\n",
    "        self.L,self.low=cho_factor(KXX)\n",
    "        self.coef=cho_solve((self.L,self.low),y_tr,check_finite=False)\n",
    "        #self.KXX_inv=np.linalg.inv(KXX)\n",
    "        #self.coef=np.matmul(self.KXX_inv,(y_train-self.yp).reshape(-1,1))\n",
    "        return self\n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        \"Predict values with features\"\n",
    "        KQX=self.kernel.matrix(x_test,self.x_train)\n",
    "        return self.yp+np.matmul(KQX,self.coef)\n",
    "    \n",
    "    def uncertainty(self,x_test):\n",
    "        #KQQ=self.kernel.matrix(x_test,x_test)\n",
    "        KQX=self.kernel.matrix(x_test,self.x_train)\n",
    "        var=self.kernel.k0-np.einsum('ij,ji->i',KQX,cho_solve((self.L,self.low),KQX.T,check_finite=False))\n",
    "        return np.sqrt(var)\n",
    "        #return np.sqrt(np.diagonal(KQQ)-np.einsum('ij,ji->i',KQX,np.matmul(self.KXX_inv,KQX.T)))\n",
    "        #return np.sqrt(np.diagonal(KQQ)-np.diagonal(np.matmul(KQX,np.matmul(self.KXX_inv,KQX.T)))) \n",
    "    \n",
    "    def error(self,x_test,y_test):\n",
    "        \"Calculate the squared error\"\n",
    "        return np.sum((y_test-self.predict(x_test))**2)\n",
    "    \n",
    "    def int_error(self,x_test,y_test):\n",
    "        return np.trapz((y_test.flatten()-self.predict(x_test).flatten())**2,x_test)\n",
    "    \n",
    "    \n",
    "    def lml(self,theta,x_train,y_train,dis_m=None):\n",
    "        length,noise,k0=theta\n",
    "        self.kernel.length=length\n",
    "        self.kernel.k0=k0\n",
    "        self.noise=noise\n",
    "        y_tr=(y_train-self.yp).reshape(-1,1)\n",
    "        C=self.kernel.matrix(x_train,x_train,dis_m=dis_m)+self.noise*np.identity(len(y_tr))\n",
    "        #C_inv=np.linalg.inv(C)\n",
    "        #coef=np.matmul(C_inv,y_tr)\n",
    "        L,low=cho_factor(C)\n",
    "        coef=cho_solve((L,low),y_train,check_finite=False)\n",
    "        return -0.5*(np.matmul(y_tr.flatten(),coef)+2*np.sum(np.log(np.diagonal(L)))+len(y_train)*np.log(2*np.pi)).item(0)\n",
    "        #if np.linalg.slogdet(C)[0]<0:\n",
    "        #    print(np.linalg.slogdet(C))\n",
    "        #return -0.5*(np.matmul(y_tr.flatten(),coef)+np.linalg.slogdet(C)[1]+len(y_train)*np.log(2*np.pi)).item(0)\n",
    "    \n",
    "    def nmml(self,theta,x_train,y_train,dis_m=None):\n",
    "        lml=self.lml(theta,x_train,y_train,dis_m=dis_m)\n",
    "        return -np.exp(lml/len(y_train))\n",
    "    \n",
    "    def optimize(self,x_train,y_train,maxfun=2000):\n",
    "        theta=[self.kernel.length,self.noise,self.kernel.k0]\n",
    "        sol=scipy.optimize.dual_annealing(self.nmml,x0=theta,bounds=np.array([[1e-3,1e3],[1e-5,1e2],[1e-3,1e3]]),maxfun=maxfun,args=(x_train,y_train,None))\n",
    "        theta_o=sol.x\n",
    "        self.kernel.length=theta_o[0]\n",
    "        self.kernel.k0=theta_o[-1]\n",
    "        self.noise=theta_o[1]\n",
    "        self.train(x_train,y_train)\n",
    "        return theta_o\n",
    "    \n",
    "    def update(self,length=None,k0=None,noise=None,yp=None):\n",
    "        if length is not None:\n",
    "            self.kernel.length=length\n",
    "        if k0 is not None:\n",
    "            self.kernel.k0=k0\n",
    "        if noise is not None:\n",
    "            self.noise=noise\n",
    "        if yp is not None:\n",
    "            self.yp=yp\n",
    "        return self\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
